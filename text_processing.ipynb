{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7a90835-9aff-4ee8-8a45-07f37d298381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: PyMuPDF in c:\\users\\hp\\appdata\\roaming\\python\\python311\\site-packages (1.25.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# First, ensure you have the necessary library installed. Run this cell.\n",
    "!pip install PyMuPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "854f02a2-d100-4ae3-96ae-4a65afa10333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting text extraction process...\n",
      "-> Processing: privrulepd.pdf...\n",
      "MuPDF error: format error: cmsOpenProfileFromMem failed\n",
      "\n",
      "-> Processing: ccpa_statute.pdf...\n",
      "-> Processing: CELEX_02016R0679-20160504_EN_TXT.pdf...\n",
      "\n",
      "Extraction complete!\n",
      "--------------------\n",
      "\n",
      "--- Sample from: privrulepd.pdf ---\n",
      "VerDate Aug<2,>2002 \n",
      "19:04 Aug 13, 2002\n",
      "Jkt 197001\n",
      "PO 00000\n",
      "Frm 00001\n",
      "Fmt 4717\n",
      "Sfmt 4717\n",
      "E:\\FR\\FM\\14AUR4.SGM\n",
      "pfrm17\n",
      "PsN: 14AUR4\"1985 National Archives and Records Administration\" seal\n",
      "Wednesday, \n",
      "August 14, 2002 \n",
      "Part V \n",
      "Department of \n",
      "Health and Human \n",
      "Services \n",
      "Office of the Secretary \n",
      "45 CFR Parts 160 and 164 \n",
      "Standards for Privacy of Individually \n",
      "Identifiable Health Information; Final \n",
      "Rule \n",
      " \n",
      " \n",
      " \n",
      "VerDate Aug<2,>2002 \n",
      "19:04 Aug 13, 2002\n",
      "Jkt 197001\n",
      "PO 00000\n",
      "Frm 00002\n",
      "Fmt 4701\n",
      "Sfmt 4700\n",
      "E:\\FR...\n",
      "--------------------\n",
      "\n",
      "--- Sample from: ccpa_statute.pdf ---\n",
      "Page 1 of 65 \n",
      "CALIFORNIA CONSUMER PRIVACY ACT OF 2018 \n",
      "effective 01/01/2025 – SB 1223, AB 1008, AB 1824 update \n",
      "posted to cppa.ca.gov January 2025 \n",
      "Contents \n",
      "1798.100.  General Duties of Businesses that Collect Personal Information................................. 3 \n",
      "1798.105.  Consumers’ Right to Delete Personal Information....................................................... 4 \n",
      "1798.106.  Consumers’ Right to Correct Inaccurate Personal Information ................................... 6 \n",
      "1798....\n",
      "--------------------\n",
      "\n",
      "--- Sample from: CELEX_02016R0679-20160504_EN_TXT.pdf ---\n",
      "This text is meant purely as a documentation tool and has no legal effect. The Union's institutions do not assume any liability \n",
      "for its contents. The authentic versions of the relevant acts, including their preambles, are those published in the Official \n",
      "Journal of the European Union and available in EUR-Lex. Those official texts are directly accessible through the links \n",
      "embedded in this document \n",
      "►B REGULATION (EU) 2016/679 OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL \n",
      "of 27 April 2016 \n",
      "on t...\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "# Now, run this cell to extract the text from all your PDF files.\n",
    "import fitz  # This is the PyMuPDF library\n",
    "import os\n",
    "\n",
    "# --- Configuration ---\n",
    "# List of the PDF files in your Jupyter directory\n",
    "pdf_filenames = [\n",
    "    \"privrulepd.pdf\",                          # HIPAA Rule\n",
    "    \"ccpa_statute.pdf\",                        # CCPA Statute\n",
    "    \"CELEX_02016R0679-20160504_EN_TXT.pdf\"   # GDPR Text\n",
    "]\n",
    "\n",
    "# --- Function Definition ---\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    Opens a PDF file and extracts the full text content from all pages.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        full_text = \"\"\n",
    "        # Iterate through each page of the PDF\n",
    "        for page in doc:\n",
    "            full_text += page.get_text()\n",
    "        doc.close()\n",
    "        return full_text\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {pdf_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# --- Main Execution Logic ---\n",
    "# A dictionary to store the extracted text, with filenames as keys\n",
    "extracted_texts = {}\n",
    "\n",
    "print(\"Starting text extraction process...\")\n",
    "\n",
    "# Loop through each filename, extract text, and store it\n",
    "for filename in pdf_filenames:\n",
    "    if os.path.exists(filename):\n",
    "        print(f\"-> Processing: {filename}...\")\n",
    "        extracted_texts[filename] = extract_text_from_pdf(filename)\n",
    "    else:\n",
    "        print(f\"-> File not found: {filename}. Please check the name.\")\n",
    "\n",
    "print(\"\\nExtraction complete!\")\n",
    "\n",
    "# --- Verification Step ---\n",
    "# Print the first 500 characters of each extracted document to verify\n",
    "print(\"-\" * 20)\n",
    "for filename, text in extracted_texts.items():\n",
    "    print(f\"\\n--- Sample from: {filename} ---\")\n",
    "    if text:\n",
    "        print(text[:500] + \"...\")\n",
    "    else:\n",
    "        print(\"No text was extracted.\")\n",
    "    print(\"-\" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5500aba7-7f2d-466a-af50-490015f40255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting text cleaning process...\n",
      "-> Applying HIPAA cleaning rules for privrulepd.pdf...\n",
      "-> Applying CCPA cleaning rules for ccpa_statute.pdf...\n",
      "-> Applying GDPR cleaning rules for CELEX_02016R0679-20160504_EN_TXT.pdf...\n",
      "\n",
      "Cleaning complete!\n",
      "--------------------\n",
      "\n",
      "--- Cleaned Sample from: privrulepd.pdf ---\n",
      "19:04 Aug 13, 2002\n",
      "Jkt 197001\n",
      "PO 00000\n",
      "Frm 00001\n",
      "Fmt 4717\n",
      "Sfmt 4717\n",
      "\n",
      "pfrm17\n",
      "PsN: 14AUR4\"1985 National Archives and Records Administration\" seal\n",
      ". \n",
      "List of Subjects \n",
      "45 CFR Part 160 \n",
      "Electronic transactions, Employer \n",
      "benefit plan, Health, Health care, Health \n",
      "facilities, Health insurance, Health \n",
      "records, Medicaid, Medical research, \n",
      "Medicare, Privacy, Reporting and record \n",
      "keeping requirements. \n",
      "45 CFR Part 164 \n",
      "Electronic transactions, Employer \n",
      "benefit plan, Health, Health care, Health \n",
      "facilities, Health insurance, Health \n",
      "records, Medicaid, Medical research, \n",
      "Medicare, Privacy, Reporting and record \n",
      "keeping requirements. \n",
      "Dated: August 6, 2002. \n",
      "Tommy G. Thompson, \n",
      "Secretary. \n",
      "For the reasons set forth in the \n",
      "preamble, the Department amends 45 \n",
      "CFR subtitle A, subchapter C, as \n",
      "follows: \n",
      "PART 160—GENERAL \n",
      "ADMINISTRATIVE REQUIREMENTS \n",
      "1. The authority citation for part 160 \n",
      "continues to read as follows: \n",
      "Authority: Sec. 1171 through 1179 of the \n",
      "Social Security Act (42 U.S.C. 1320...\n",
      "--------------------\n",
      "\n",
      "--- Cleaned Sample from: ccpa_statute.pdf ---\n",
      "1798.100. General Duties of Businesses that Collect Personal Information................................. 3 \n",
      "1798.105. Consumers’ Right to Delete Personal Information....................................................... 4 \n",
      "1798.106. Consumers’ Right to Correct Inaccurate Personal Information ................................... 6 \n",
      "1798.110. Consumers’ Right to Know What Personal Information is Being Collected. Right to \n",
      "Access Personal Information .......................................................................................................... 6 \n",
      "1798.115. Consumers’ Right to Know What Personal Information is Sold or Shared and to \n",
      "Whom.............................................................................................................................................. 7 \n",
      "1798.120. Consumers’ Right to Opt Out of Sale or Sharing of Personal Information................... 8 \n",
      "1798.121. Consumers’ Right to Limit Use and Disclosure of Sensitive Personal Information ...... 9...\n",
      "--------------------\n",
      "\n",
      "--- Cleaned Sample from: CELEX_02016R0679-20160504_EN_TXT.pdf ---\n",
      "REGULATION (EU) 2016/679 OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL \n",
      "of 27 April 2016 \n",
      "on the protection of natural persons with regard to the processing of personal data and on the free \n",
      "movement of such data, and repealing Directive 95/46/EC (General Data Protection Regulation) \n",
      "(Text with EEA relevance) \n",
      "(OJ L 119, 4.5.2016, p. 1) \n",
      "Corrected by: \n",
      "\n",
      "Corrigendum, OJ L 127, 23.5.2018, p. 2 (2016/679) \n",
      "02016R0679 — EN — 04.05.2016 — 000.002 — 1\n",
      "02016R0679 — EN — 04.05.2016 — 000.002 — 2 \n",
      "REGULATION \n",
      "(EU) \n",
      "2016/679 \n",
      "OF \n",
      "THE \n",
      "EUROPEAN \n",
      "PARLIAMENT AND OF THE COUNCIL \n",
      "of 27 April 2016 \n",
      "on the protection of natural persons with regard to the processing \n",
      "of personal data and on the free movement of such data, and \n",
      "repealing Directive 95/46/EC (General Data Protection Regulation) \n",
      "(Text with EEA relevance) \n",
      "CHAPTER I \n",
      "General provisions \n",
      "Article 1 \n",
      "Subject-matter and objectives \n",
      "1. \n",
      "This Regulation lays down rules relating to the protection of \n",
      "natural persons with regard to the processing o...\n",
      "--------------------\n",
      "\n",
      "Cleaned text has been saved to .txt files for your review.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "# --- Cleaning Function Definitions ---\n",
    "\n",
    "def clean_hipaa_text(text):\n",
    "    \"\"\"Applies specific cleaning rules for the HIPAA PDF (privrulepd.pdf).\"\"\"\n",
    "    # Remove Federal Register headers and footers\n",
    "    text = re.sub(r'Federal Register / Vol\\. \\d+, No\\. \\d+ /.+', '', text)\n",
    "    # Remove garbled extraction metadata (e.g., VerDate, Jkt, Fmt)\n",
    "    text = re.sub(r'VerDate.+', '', text)\n",
    "    # Remove page numbers that are on their own line\n",
    "    text = re.sub(r'^\\s*\\d+\\s*$', '', text, flags=re.MULTILINE)\n",
    "    # Remove content from the first page's seal and header\n",
    "    text = re.sub(r'ARCHIVES AND RECORDS.*ADMINISTRATION', '', text, flags=re.DOTALL)\n",
    "    text = re.sub(r'Wednesday,.*Rule', '', text, flags=re.DOTALL)\n",
    "    # A more general pattern to catch file metadata lines\n",
    "    text = re.sub(r'E:\\\\FR\\\\FM\\\\.+', '', text)\n",
    "    return text\n",
    "\n",
    "def clean_ccpa_text(text):\n",
    "    \"\"\"Applies specific cleaning rules for the CCPA PDF (ccpa_statute.pdf).\"\"\"\n",
    "    # Remove \"Page X of 65\" footers\n",
    "    text = re.sub(r'Page \\d+ of \\d+', '', text)\n",
    "    # Remove the initial title and contents section\n",
    "    text = re.sub(r'CALIFORNIA CONSUMER PRIVACY ACT OF 2018.*?(?=1798\\.100\\.)', '', text, flags=re.DOTALL)\n",
    "    return text\n",
    "\n",
    "def clean_gdpr_text(text):\n",
    "    \"\"\"Applies specific cleaning rules for the GDPR PDF (CELEX...).\"\"\"\n",
    "    # Remove the disclaimer header on the first page\n",
    "    text = re.sub(r'This text is meant purely as a documentation tool.*?embedded in this document', '', text, flags=re.DOTALL)\n",
    "    # Remove regulation code headers\n",
    "    text = re.sub(r'02016R0679-EN-.*?\\d+-\\d+', '', text)\n",
    "    # Remove artifacts like ►B, ▼B, ►C1 etc.\n",
    "    text = re.sub(r'►[A-Z]\\d*|▼[A-Z]\\d*', '', text)\n",
    "    return text\n",
    "\n",
    "def clean_general_text(text):\n",
    "    \"\"\"Applies general cleaning rules to any text.\"\"\"\n",
    "    # Normalize whitespace: replace multiple spaces/tabs with a single space\n",
    "    text = re.sub(r'[ \\t]+', ' ', text)\n",
    "    # Replace multiple newlines with a double newline to preserve paragraph structure\n",
    "    text = re.sub(r'\\n\\s*\\n', '\\n\\n', text)\n",
    "    # Remove leading/trailing whitespace from the whole text\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "# --- Main Execution Logic ---\n",
    "# A dictionary to hold the cleaned text\n",
    "cleaned_texts = {}\n",
    "\n",
    "print(\"Starting text cleaning process...\")\n",
    "\n",
    "# Get the filenames from the previous step\n",
    "pdf_filenames = list(extracted_texts.keys())\n",
    "\n",
    "for filename in pdf_filenames:\n",
    "    raw_text = extracted_texts[filename]\n",
    "    \n",
    "    # Apply specific cleaning function based on the filename\n",
    "    if \"privrulepd.pdf\" in filename:\n",
    "        print(f\"-> Applying HIPAA cleaning rules for {filename}...\")\n",
    "        cleaned_text = clean_hipaa_text(raw_text)\n",
    "    elif \"ccpa_statute.pdf\" in filename:\n",
    "        print(f\"-> Applying CCPA cleaning rules for {filename}...\")\n",
    "        cleaned_text = clean_ccpa_text(raw_text)\n",
    "    elif \"CELEX\" in filename: # Using a unique part of the GDPR filename\n",
    "        print(f\"-> Applying GDPR cleaning rules for {filename}...\")\n",
    "        cleaned_text = clean_gdpr_text(raw_text)\n",
    "    else:\n",
    "        cleaned_text = raw_text # No specific rules for this file\n",
    "        \n",
    "    # Apply general cleaning to the result of the specific cleaning\n",
    "    cleaned_texts[filename] = clean_general_text(cleaned_text)\n",
    "\n",
    "print(\"\\nCleaning complete!\")\n",
    "\n",
    "\n",
    "# --- Verification Step ---\n",
    "# Print the first 1000 characters of each cleaned document to verify the changes\n",
    "print(\"-\" * 20)\n",
    "for filename, text in cleaned_texts.items():\n",
    "    print(f\"\\n--- Cleaned Sample from: {filename} ---\")\n",
    "    if text:\n",
    "        print(text[:1000] + \"...\")\n",
    "    else:\n",
    "        print(\"No text after cleaning.\")\n",
    "    print(\"-\" * 20)\n",
    "\n",
    "# Optional: Save the cleaned text to files for inspection\n",
    "for filename, text in cleaned_texts.items():\n",
    "    output_filename = filename.replace('.pdf', '_cleaned.txt')\n",
    "    with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "        f.write(text)\n",
    "print(\"\\nCleaned text has been saved to .txt files for your review.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98ff6568-9609-4e51-9d9e-ad794ab1f7ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting langchain\n",
      "  Downloading langchain-0.3.27-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting langchain-core<1.0.0,>=0.3.72 (from langchain)\n",
      "  Downloading langchain_core-0.3.76-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting langchain-text-splitters<1.0.0,>=0.3.9 (from langchain)\n",
      "  Downloading langchain_text_splitters-0.3.11-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting langsmith>=0.1.17 (from langchain)\n",
      "  Downloading langsmith-0.4.27-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting pydantic<3.0.0,>=2.7.4 (from langchain)\n",
      "  Downloading pydantic-2.11.7-py3-none-any.whl.metadata (67 kB)\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain)\n",
      "  Downloading sqlalchemy-2.0.43-cp311-cp311-win_amd64.whl.metadata (9.8 kB)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\hp\\appdata\\roaming\\python\\python311\\site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\hp\\appdata\\roaming\\python\\python311\\site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\hp\\appdata\\roaming\\python\\python311\\site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (9.0.0)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<1.0.0,>=0.3.72->langchain)\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\hp\\appdata\\roaming\\python\\python311\\site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (4.12.2)\n",
      "Requirement already satisfied: packaging>=23.2 in c:\\users\\hp\\appdata\\roaming\\python\\python311\\site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (24.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\hp\\appdata\\roaming\\python\\python311\\site-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
      "Collecting orjson>=3.9.14 (from langsmith>=0.1.17->langchain)\n",
      "  Downloading orjson-3.11.3-cp311-cp311-win_amd64.whl.metadata (43 kB)\n",
      "Collecting requests-toolbelt>=1.0.0 (from langsmith>=0.1.17->langchain)\n",
      "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Collecting zstandard>=0.23.0 (from langsmith>=0.1.17->langchain)\n",
      "  Downloading zstandard-0.24.0-cp311-cp311-win_amd64.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\hp\\appdata\\roaming\\python\\python311\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
      "  Downloading pydantic_core-2.33.2-cp311-cp311-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
      "  Downloading typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hp\\appdata\\roaming\\python\\python311\\site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\appdata\\roaming\\python\\python311\\site-packages (from requests<3,>=2->langchain) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\appdata\\roaming\\python\\python311\\site-packages (from requests<3,>=2->langchain) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\appdata\\roaming\\python\\python311\\site-packages (from requests<3,>=2->langchain) (2023.11.17)\n",
      "Collecting greenlet>=1 (from SQLAlchemy<3,>=1.4->langchain)\n",
      "  Downloading greenlet-3.2.4-cp311-cp311-win_amd64.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: anyio in c:\\users\\hp\\appdata\\roaming\\python\\python311\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (4.2.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\hp\\appdata\\roaming\\python\\python311\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\hp\\appdata\\roaming\\python\\python311\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\hp\\appdata\\roaming\\python\\python311\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (2.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\hp\\appdata\\roaming\\python\\python311\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.0)\n",
      "Downloading langchain-0.3.27-py3-none-any.whl (1.0 MB)\n",
      "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 0.3/1.0 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 0.3/1.0 MB ? eta -:--:--\n",
      "   -------------------- ------------------- 0.5/1.0 MB 799.2 kB/s eta 0:00:01\n",
      "   ------------------------------ --------- 0.8/1.0 MB 860.9 kB/s eta 0:00:01\n",
      "   ------------------------------ --------- 0.8/1.0 MB 860.9 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.0/1.0 MB 722.8 kB/s eta 0:00:00\n",
      "Downloading langchain_core-0.3.76-py3-none-any.whl (447 kB)\n",
      "Downloading langchain_text_splitters-0.3.11-py3-none-any.whl (33 kB)\n",
      "Downloading langsmith-0.4.27-py3-none-any.whl (384 kB)\n",
      "Downloading pydantic-2.11.7-py3-none-any.whl (444 kB)\n",
      "Downloading pydantic_core-2.33.2-cp311-cp311-win_amd64.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.3/2.0 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.3/2.0 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 0.5/2.0 MB 1.1 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 0.8/2.0 MB 1.1 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 1.0/2.0 MB 1.1 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 1.3/2.0 MB 1.1 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 1.3/2.0 MB 1.1 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.6/2.0 MB 975.2 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.0/2.0 MB 1.0 MB/s eta 0:00:00\n",
      "Downloading sqlalchemy-2.0.43-cp311-cp311-win_amd64.whl (2.1 MB)\n",
      "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.3/2.1 MB ? eta -:--:--\n",
      "   --------- ------------------------------ 0.5/2.1 MB 985.5 kB/s eta 0:00:02\n",
      "   -------------- ------------------------- 0.8/2.1 MB 987.4 kB/s eta 0:00:02\n",
      "   ------------------- -------------------- 1.0/2.1 MB 1.1 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 1.3/2.1 MB 1.0 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 1.3/2.1 MB 1.0 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.8/2.1 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.1/2.1 MB 1.2 MB/s eta 0:00:00\n",
      "Downloading greenlet-3.2.4-cp311-cp311-win_amd64.whl (299 kB)\n",
      "Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Downloading orjson-3.11.3-cp311-cp311-win_amd64.whl (131 kB)\n",
      "Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Downloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Downloading zstandard-0.24.0-cp311-cp311-win_amd64.whl (505 kB)\n",
      "Installing collected packages: zstandard, typing-inspection, pydantic-core, orjson, jsonpatch, greenlet, SQLAlchemy, requests-toolbelt, pydantic, langsmith, langchain-core, langchain-text-splitters, langchain\n",
      "  Attempting uninstall: pydantic-core\n",
      "    Found existing installation: pydantic_core 2.18.4\n",
      "    Uninstalling pydantic_core-2.18.4:\n",
      "      Successfully uninstalled pydantic_core-2.18.4\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 2.7.3\n",
      "    Uninstalling pydantic-2.7.3:\n",
      "      Successfully uninstalled pydantic-2.7.3\n",
      "Successfully installed SQLAlchemy-2.0.43 greenlet-3.2.4 jsonpatch-1.33 langchain-0.3.27 langchain-core-0.3.76 langchain-text-splitters-0.3.11 langsmith-0.4.27 orjson-3.11.3 pydantic-2.11.7 pydantic-core-2.33.2 requests-toolbelt-1.0.0 typing-inspection-0.4.1 zstandard-0.24.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Install the library needed for the text splitter\n",
    "!pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8bd0d7b-3ad8-4305-940e-21ba30507ca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MuPDF error: format error: cmsOpenProfileFromMem failed\n",
      "\n",
      "Starting final, refined processing...\n",
      "\n",
      "Refined processing complete!\n",
      "Total chunks created: 1376\n",
      "\n",
      "--- Sample of CORRECT final structured data ---\n",
      "{\n",
      "  \"text\": \"on August 21\\u201323, 2001, and January 24\\u2013 \\n25, 2002, and provided \\nrecommendations to the Department \\nbased on these hearings. The NCVHS \\nserves as the statutory advisory body to \\nthe Secretary of HHS with respect to the \\ndevelopment and implementation of the \\nRules required by the Administrative \\nSimplification provisions of HIPAA, \\nincluding the privacy standards. \\nThrough the hearings, the NCVHS \\nspecifically solicited public input on \\nissues related to certain key standards in \\nthe Privacy Rule: consent, minimum \\nnecessary, marketing, fundraising, and \\nresearch. The resultant public testimony \\nand subsequent recommendations \\nsubmitted to the Department by the \\nNCVHS also served to inform the \\ndevelopment of these proposed \\nmodifications. \\nII. Overview of the March 2002 Notice \\nof Proposed Rulemaking (NPRM) \\nAs described above, through public \\ncomments, testimony at public hearings, \\nmeetings at the request of industry and \\nother stakeholders, as well as other\",\n",
      "  \"metadata\": {\n",
      "    \"source_document\": \"privrulepd.pdf\",\n",
      "    \"heading\": \"B. Regulatory and Other Actions to Date\"\n",
      "  }\n",
      "}\n",
      "--------------------\n",
      "{\n",
      "  \"text\": \"maintained by a covered entity in its \\ncapacity as an employer from the \\ndefinition of \\u2018\\u2018protected health \\ninformation.\\u2019\\u2019 The Department agrees \\nwith commenters that the regulation \\nshould be explicit that it does not apply \\nto a covered entity\\u2019s employer functions \\nand that the most effective means of \\naccomplishing this is through the \\ndefinition of \\u2018\\u2018protected health \\ninformation.\\u2019\\u2019 \\nThe Department is sensitive to the \\nconcerns of commenters that a covered \\nentity not abuse its access to an \\nemployee\\u2019s individually identifiable \\nhealth information which it has created \\nor maintains in its health care, not its \\nemployer, capacity. In responding to \\nthese concerns, the Department must \\nremain within the boundaries set by the \\nstatute, which does not include \\nemployers per se as covered entities. \\nThus, we cannot regulate employers, \\neven when it is a covered entity acting \\nas an employer. \\nTo address these concerns, the \\nDepartment clarifies that a covered\",\n",
      "  \"metadata\": {\n",
      "    \"source_document\": \"privrulepd.pdf\",\n",
      "    \"heading\": \"A. Section 164.501\\u2014Definitions\"\n",
      "  }\n",
      "}\n",
      "--------------------\n",
      "{\n",
      "  \"text\": \"VerDate Aug<2,>2002 \\n19:04 Aug 13, 2002\\nJkt 197001\\nPO 00000\\nFrm 00022\\nFmt 4701\\nSfmt 4700\\nE:\\\\FR\\\\FM\\\\14AUR4.SGM\\npfrm17\\nPsN: 14AUR4\",\n",
      "  \"metadata\": {\n",
      "    \"source_document\": \"privrulepd.pdf\",\n",
      "    \"heading\": \"B. Section 164.502\\u2014Uses and\"\n",
      "  }\n",
      "}\n",
      "--------------------\n",
      "\n",
      "Successfully saved all chunks to processed_chunks.jsonl. Phase 1 is now truly complete!\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import re\n",
    "import json\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# --- Step 1: Reload Original Text (to start fresh) ---\n",
    "pdf_filenames = [\n",
    "    \"privrulepd.pdf\",\n",
    "    \"ccpa_statute.pdf\",\n",
    "    \"CELEX_02016R0679-20160504_EN_TXT.pdf\"\n",
    "]\n",
    "\n",
    "extracted_texts = {}\n",
    "for filename in pdf_filenames:\n",
    "    doc = fitz.open(filename)\n",
    "    full_text = \"\"\n",
    "    for page in doc:\n",
    "        full_text += page.get_text()\n",
    "    doc.close()\n",
    "    extracted_texts[filename] = full_text\n",
    "\n",
    "# --- Step 2: Improved, More Aggressive Cleaning ---\n",
    "\n",
    "def final_clean_text(text, doc_type):\n",
    "    \"\"\"Applies a final, robust cleaning pass.\"\"\"\n",
    "    \n",
    "    # General cleaning\n",
    "    text = re.sub(r'[ \\t]+', ' ', text) # Normalize whitespace\n",
    "    text = re.sub(r'\\n\\s*\\n', '\\n\\n', text) # Normalize paragraph breaks\n",
    "    \n",
    "    if doc_type == 'hipaa':\n",
    "        # Remove headers, footers, and initial metadata\n",
    "        text = re.sub(r'Federal Register / Vol\\. \\d+, No\\. \\d+ /.+', '', text)\n",
    "        text = re.sub(r'^\\s*\\d+\\s*$', '', text, flags=re.MULTILINE) # Page numbers\n",
    "        start_match = re.search(r'I\\. Background', text)\n",
    "        if start_match:\n",
    "            text = text[start_match.start():]\n",
    "            \n",
    "    elif doc_type == 'ccpa':\n",
    "        # Aggressively remove the entire table of contents\n",
    "        text = re.sub(r'Contents.*?(?=1798\\.100\\. General Duties)', '', text, flags=re.DOTALL)\n",
    "        text = re.sub(r'Page \\d+ of \\d+', '', text) # Remove page footers\n",
    "        \n",
    "    elif doc_type == 'gdpr':\n",
    "        # Remove disclaimer and document codes\n",
    "        text = re.sub(r'This text is meant purely as a documentation tool.*?(?=CHAPTER I)', '', text, flags=re.DOTALL)\n",
    "        text = re.sub(r'L \\d+/\\d+', '', text)\n",
    "        text = re.sub(r'[\\(][\\d]+[\\)]', '', text) # remove numbers in brackets\n",
    "        text = re.sub(r'►[A-Z]\\d*|▼[A-Z]\\d*', '', text) # Remove artifacts\n",
    "        \n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "# --- Step 3: Robust Chunking and Metadata Extraction ---\n",
    "\n",
    "heading_patterns = {\n",
    "    'hipaa': r'^[A-Z]\\. .+',\n",
    "    'ccpa': r'^\\d{4}\\.\\d{3,}\\..+',\n",
    "    'gdpr': r'^Article \\d+\\n'\n",
    "}\n",
    "\n",
    "doc_types = {\n",
    "    \"privrulepd.pdf\": 'hipaa',\n",
    "    \"ccpa_statute.pdf\": 'ccpa',\n",
    "    \"CELEX_02016R0679-20160504_EN_TXT.pdf\": 'gdpr'\n",
    "}\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=150,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    ")\n",
    "\n",
    "final_chunks = []\n",
    "print(\"Starting final, refined processing...\")\n",
    "\n",
    "for filename, raw_text in extracted_texts.items():\n",
    "    doc_type = doc_types[filename]\n",
    "    \n",
    "    # Apply final cleaning\n",
    "    clean_text = final_clean_text(raw_text, doc_type)\n",
    "    \n",
    "    # Find all headings\n",
    "    heading_pattern = heading_patterns[doc_type]\n",
    "    headings = [match.group(0).strip() for match in re.finditer(heading_pattern, clean_text, re.MULTILINE)]\n",
    "    \n",
    "    # Split the document by these headings\n",
    "    sections = re.split(heading_pattern, clean_text, flags=re.MULTILINE)\n",
    "    \n",
    "    # The first element is the text before the first heading\n",
    "    doc_intro = sections[0].strip()\n",
    "    if doc_intro:\n",
    "        intro_chunks = text_splitter.split_text(doc_intro)\n",
    "        for chunk in intro_chunks:\n",
    "            final_chunks.append({\"text\": chunk, \"metadata\": {\"source_document\": filename, \"heading\": \"Introduction\"}})\n",
    "\n",
    "    # Process text under each heading\n",
    "    for i, section_text in enumerate(sections[1:]):\n",
    "        section_text = section_text.strip()\n",
    "        if not section_text:\n",
    "            continue\n",
    "        \n",
    "        heading = headings[i]\n",
    "        chunks = text_splitter.split_text(section_text)\n",
    "        for chunk in chunks:\n",
    "            final_chunks.append({\"text\": chunk, \"metadata\": {\"source_document\": filename, \"heading\": heading}})\n",
    "\n",
    "print(\"\\nRefined processing complete!\")\n",
    "\n",
    "# --- Verification Step ---\n",
    "print(f\"Total chunks created: {len(final_chunks)}\")\n",
    "print(\"\\n--- Sample of CORRECT final structured data ---\")\n",
    "# Print a few representative chunks\n",
    "print(json.dumps(final_chunks[10], indent=2))\n",
    "print(\"-\" * 20)\n",
    "print(json.dumps(final_chunks[100], indent=2))\n",
    "print(\"-\" * 20)\n",
    "print(json.dumps(final_chunks[200], indent=2))\n",
    "print(\"-\" * 20)\n",
    "\n",
    "# --- Save the final output ---\n",
    "output_filename = \"processed_chunks.jsonl\"\n",
    "with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "    for chunk in final_chunks:\n",
    "        f.write(json.dumps(chunk) + '\\n')\n",
    "\n",
    "print(f\"\\nSuccessfully saved all chunks to {output_filename}. Phase 1 is now truly complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
